{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ed849e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumerKey = “Type your consumer key here”\n",
    "consumerSecret = “Type your consumer secret here”\n",
    "accessToken = “Type your accedd token here”\n",
    "accessTokenSecret = “Type your access token secret here”auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "112e7811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Successfully!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print('Imported Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ff304a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from .autonotebook import tqdm as notebook_tqdm\n",
    "from keybert import KeyBERT\n",
    "\n",
    "doc = \"\"\"\n",
    "       Just another cute  #travelcoffeemug to add to my mug collection... as if I didn't have enough already\n",
    "      \"\"\"\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "df544678",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=kw_model.extract_keywords(doc, keyphrase_ngram_range=(2, 2), stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "77c19104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cute travelcoffeemug', 0.6747),\n",
       " ('mug collection', 0.6129),\n",
       " ('my mug', 0.5678),\n",
       " ('travelcoffeemug to', 0.5525),\n",
       " ('another cute', 0.3371)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f993a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "corpus = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "636eebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "747128f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mug', 0.6172524094581604), ('mugs', 0.5745022296905518), ('Styrofoam_cup', 0.56965571641922), ('coffee_mugs', 0.557625949382782), ('ceramic_mug', 0.5331496596336365)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=['coffee_mug'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8c7d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/achyuth/opt/anaconda3/envs/pyspark_3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "987bd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ents(doc): \n",
    "    if doc.ents: \n",
    "        for ent in doc.ents: \n",
    "            print(ent.text+' - ' +str(ent.start_char) +' - '+ str(ent.end_char) +' - '+ent.label_+ ' - '+str(spacy.explain(ent.label_))) \n",
    "    else: print('No named entities found.')\n",
    "doc1 = nlp(\"An ultra-insulated mug can make a world of difference in keeping your liquids chilled for a long time!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b29bf27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "An ultra-insulated mug can make a world of difference in keeping your liquids chilled for a long time!"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c1ad8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No named entities found.\n"
     ]
    }
   ],
   "source": [
    "show_ents(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c02aee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'flair.data.Sentence'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "# create a sentence #\n",
    "sentence = Sentence('Blogs of Analytics Vidhya are Awesome.')\n",
    "# print the sentence to see what’s in it. #\n",
    "print(Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08583304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-13 15:27:31,235 loading file /Users/achyuth/.flair/models/ner-english/4f4cdab26f24cb98b732b389e6cebc646c36f54cfd6e0b7d3b90b25656e4262f.8baa8ae8795f4df80b28e7f7b61d788ecbb057d1dc85aacb316f1bd02837a4a4\n",
      "2022-07-13 15:27:34,775 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('Can I please borrow 500 dollars from you to buy some Microsoft stock?')\n",
    "\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load('ner')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3b72851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Can I please borrow 500 dollars from you to buy some Microsoft stock ?\" → [\"Microsoft\"/ORG]\n",
      "The following NER tags are found:\n",
      "Span[11:12]: \"Microsoft\" → ORG (0.9869)\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "\n",
    "print('The following NER tags are found:')\n",
    "\n",
    "# iterate over entities and print each\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34b5a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "parsed_text = nlp(u\"Who remembers stainless cup? Good old days\")\n",
    "\n",
    "#get token dependencies\n",
    "for text in parsed_text:\n",
    "    #subject would be\n",
    "    if text.dep_ == \"nsubj\":\n",
    "        subject = text.orth_\n",
    "    #iobj for indirect object\n",
    "    if text.dep_ == \"iobj\":\n",
    "        indirect_object = text.orth_\n",
    "    #dobj for direct object\n",
    "    if text.dep_ == \"dobj\":\n",
    "        direct_object = text.orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d412625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who\n",
      "cup\n"
     ]
    }
   ],
   "source": [
    "print(subject)\n",
    "print(direct_object)\n",
    "#print(indirect_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3fd23d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Just', 'cute', '#', 'travelcoffeemug', 'add', 'mug', 'collection', '...', 'I']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "\n",
    "text = \"Just another cute  #travelcoffeemug to add to my mug collection... as if I didn't have enough already\"\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "25a37755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just cute # travelcoffeemug add mug collection ... I\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = (\" \").join(tokens_without_sw)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a8e74753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just cute # travelcoffeemug add mug collection ... I\n",
      "travelcoffeemug\n",
      "travelcoffeemug\n",
      "Collection\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "parsed_text = nlp(filtered_sentence)\n",
    "print(parsed_text)\n",
    "arr = []\n",
    "#get token dependencies\n",
    "for text in parsed_text:\n",
    "    #subject would be\n",
    "    if text.dep_ == \"nsubj\":\n",
    "        print(text.orth_)\n",
    "        arr.append(text.orth_)\n",
    "        subject = text.orth_\n",
    "    #iobj for indirect object\n",
    "    if text.dep_ == \"iobj\":\n",
    "        print(text.orth_)\n",
    "        arr.append(text.orth_)\n",
    "        indirect_object = text.orth_\n",
    "    #dobj for direct object\n",
    "    if text.dep_ == \"dobj\":\n",
    "        direct_object = text.orth_.capitalize()\n",
    "        arr.append(text.orth_.capitalize())\n",
    "        \n",
    "        \n",
    "print(subject)\n",
    "print(direct_object.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3381ba78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['travelcoffeemug', 'Collection']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/achyuth/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('travelcoffeemug', 'NN'), ('Collection', 'NN')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(arr)\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1144dd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     POS    TAG    Dep    POS explained        tag explained \n",
      "Liquids  NOUN   NNS    ROOT   noun                 noun, plural\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\" \".join(arr))\n",
    "print(f\"{'text':{8}} {'POS':{6}} {'TAG':{6}} {'Dep':{6}} {'POS explained':{20}} {'tag explained'} \")\n",
    "for token in doc:\n",
    "    print(f'{token.text:{8}} {token.pos_:{6}} {token.tag_:{6}} {token.dep_:{6}} {spacy.explain(token.pos_):{20}} {spacy.explain(token.tag_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e50d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
